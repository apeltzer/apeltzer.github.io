[{"authors":["apeltzer"],"categories":null,"content":"Alexander Peltzer is working as a Clinical Bioinformatics Lead in the Translational Medicine and Clinical Pharmacology (TMCP) department at Boehringer Ingelheim in Biberach an der Riss/Germany. Before this, he was coordinating the research \u0026amp; development in data science at the Quantitative Biology Center (QBiC) in Tübingen, working in maintaining and developing modern solutions for data management and analysis for various omics technologies there. He obtained a Ph.D in Bioinformatics at the University of Tübingen and the Max Planck Institute for the Science of Human History where he worked on computational methods for ancient DNA reconstruction.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5bb43277dcc6b1d6e4009de8d80d58a6","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Alexander Peltzer is working as a Clinical Bioinformatics Lead in the Translational Medicine and Clinical Pharmacology (TMCP) department at Boehringer Ingelheim in Biberach an der Riss/Germany. Before this, he was coordinating the research \u0026amp; development in data science at the Quantitative Biology Center (QBiC) in Tübingen, working in maintaining and developing modern solutions for data management and analysis for various omics technologies there.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":1599053400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599053400,"objectID":"e092bd93b5b0dfcf337f5ab65cf28b4b","permalink":"https://apeltzer.github.io/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","publishdate":"2020-09-02T15:30:00+02:00","relpermalink":"/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","section":"event","summary":"","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing","migrating","workflows"],"title":"Scalable, reproducible bioinformatics workflows using Nextflow \u0026 nf-core","type":"event"},{"authors":["Fellows Yates JA","Lamnidis TC","Borry M","Valtueña A","Fagernäs Z","Clayton S","Garcia MU","Neukamm J","Peltzer A"],"categories":null,"content":"","date":1591912800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591912800,"objectID":"f7d50cec7a53a6900f9a2177170bb087","permalink":"https://apeltzer.github.io/publication/16-eager/","publishdate":"2020-06-12T00:00:00+02:00","relpermalink":"/publication/16-eager/","section":"publication","summary":"The broadening utilisation of ancient DNA (aDNA) to address archaeological, palaeontological, and biological questions is resulting in a rising diversity in the size of laboratories and scale of analyses being performed. In the context of this heterogeneous landscape, we present nf-core/eager, an advanced and entirely redesigned pipeline for the analysis of ancient genomic data. nf-core/eager builds on existing ideas and concepts introduced in the original EAGER pipeline, and improves various aspects of the analysis procedure by employing computational frameworks such as Nextfow and nf-core. The pipeline aims to address three main themes: accessibility and adaptability to different research groups and their computing configurations, reproducibility to ensure robust analytical standards in the field, and updating the EAGER pipeline to the latest routine ancient genomic practises. This new version of EAGER has been developed within the nf-core initiative, to ensure high quality software development and maintenance support; contributing to a long-term lifecycle for the pipeline. nf-core/eager will assist in ensuring that ancient DNA sequencing data can be used by a diverse range of research groups and fields.","tags":["eager","ancient DNA","aDNA workflow","bioinformatics","data analysis","paleogenetics"],"title":"Reproducible, portable, and effcient ancient genome reconstruction with nf-core/eager","type":"publication"},{"authors":null,"categories":null,"content":"","date":1585899000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585899000,"objectID":"27cb503be2f0e5531977c8c8e450d04d","permalink":"https://apeltzer.github.io/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","publishdate":"2020-04-03T09:30:00+02:00","relpermalink":"/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","section":"event","summary":"","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing","migrating","workflows"],"title":"Scalable, reproducible bioinformatics workflows using Nextflow \u0026 nf-core","type":"event"},{"authors":["Kumuthini J","Chimenti M","Nahnsen S","Peltzer A","Meraba R","McFadyen R","Wells G","Taylor D","Maienschine-Cline M","Li, JL","Thimmapuram J","Murthy-Karuturi R","Zass L"],"categories":null,"content":"","date":1585177200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585177200,"objectID":"aca480c9d683888649a961fe791ca4d1","permalink":"https://apeltzer.github.io/publication/15-tenrules/","publishdate":"2020-03-26T00:00:00+01:00","relpermalink":"/publication/15-tenrules/","section":"publication","summary":"Life scientists are increasingly turning to high-throughput sequencing technologies in their research programs, owing to the enormous potential of these methods. In a parallel manner, the number of core facilities that provide bioinformatics support are also increasing. Notably, the generation of complex large datasets has necessitated the development of bioinformatics support core facilities that aid laboratory scientists with cost-effective and efficient data management, analysis, and interpretation. In this article, we address the challenges—related to communication, good laboratory practice, and data handling—that may be encountered in core support facilities when providing bioinformatics support, drawing on our own experiences working as support bioinformaticians on multidisciplinary research projects. Most importantly, the article proposes a list of guidelines that outline how these challenges can be preemptively avoided and effectively managed to increase the value of outputs to the end user, covering the entire research project lifecycle, including experimental design, data analysis, and management (i.e., sharing and storage). In addition, we highlight the importance of clear and transparent communication, comprehensive preparation, appropriate handling of samples and data using monitoring systems, and the employment of appropriate tools and standard operating procedures to provide effective bioinformatics support.","tags":["tenrules","bioinformatics","support","data analysis"],"title":"Ten simple rules for providing effective bioinformatics research support","type":"publication"},{"authors":["Ewels PA*","Peltzer A*","Fillinger S","Patel H","Alneberg J","Wilm A","Garcia MU","Di Tommaso P","Nahnsen S"],"categories":null,"content":"","date":1581548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581548400,"objectID":"8c7b07f3b584f96a97f7edce7a36e6dd","permalink":"https://apeltzer.github.io/publication/14-nfcore/","publishdate":"2020-02-13T00:00:00+01:00","relpermalink":"/publication/14-nfcore/","section":"publication","summary":"The standardization, portability and reproducibility of analysis pipelines are key issues within the bioinformatics community. Most bioinformatics pipelines are designed for use on-premises; as a result, the associated software dependencies and execution logic are likely to be tightly coupled with proprietary computing environments. This can make it difficult or even impossible for others to reproduce the ensuing results, which is a fundamental requirement for the validation of scientific findings. Here, we introduce the nf-core framework as a means for the development of collaborative, peer-reviewed, best-practice analysis pipelines.","tags":["nf-core","Nextflow","FAIR","data analysis"],"title":"The nf-core framework for community-curated bioinformatics pipelines","type":"publication"},{"authors":null,"categories":null,"content":"","date":1578990600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578990600,"objectID":"f47b622f051645d6af3e969ed445fc7d","permalink":"https://apeltzer.github.io/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","publishdate":"2020-01-14T09:30:00+01:00","relpermalink":"/talk/scalable-reproducible-bioinformatics-workflows-using-nextflow-nf-core/","section":"event","summary":"","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing","migrating","workflows"],"title":"Scalable, reproducible bioinformatics workflows using Nextflow \u0026 nf-core","type":"event"},{"authors":["Straub D","Blackwell N","Langarica Fuentes A","Peltzer A","Nahnsen S","Kleindienst S"],"categories":null,"content":"","date":1576710000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576710000,"objectID":"6875f432050ce5b1c576273b93b9a7b6","permalink":"https://apeltzer.github.io/publication/13-ampliseq/","publishdate":"2019-12-19T00:00:00+01:00","relpermalink":"/publication/13-ampliseq/","section":"publication","summary":"One of the major methods to identify microbial community composition, to unravel microbial population dynamics, and to explore microbial diversity in environmental samples is DNA- or RNA-based 16S rRNA (gene) amplicon sequencing. Subsequent bioinformatics analyses are required to extract valuable information from the high-throughput sequencing approach. However, manifold bioinformatics tools complicate their choice and might cause differences in data interpretation, making the selection of the pipeline a crucial step. Here, we compared the performance of most widely used 16S rRNA gene amplicon sequencing analysis tools (i.e. Mothur, QIIME1, QIIME2, and MEGAN) using mock datasets and environmental samples from contrasting terrestrial and freshwater sites. Our results showed that QIIME2 outcompeted all other investigated tools in sequence recovery (10 times less false positives), taxonomic assignments (22% better F-score) and diversity estimates (5% better assessment), while there was still room for improvement e.g. imperfect sequence recovery (recall up to 87%) or detection of additional false sequences (precision up to 72%). Furthermore, we found that microbial diversity estimates and highest abundant taxa varied among analysis pipelines (i.e. only one in five genera was shared among all analysis tools) when analyzing environmental samples, which might skew biological conclusions. Our findings were subsequently implemented in a high-performance computing conformant workflow following the FAIR (Findable, Accessible, Interoperable, and Re-usable) principle, allowing reproducible 16S rRNA gene amplicon sequence analysis starting from raw sequence files. Our presented workflow can be utilized for future studies, thereby facilitating the analysis of high-throughput DNA- or RNA-based 16S rRNA (gene) sequencing data substantially.","tags":["amplicon","16S","rRNA","amplicon sequencing","environmental omics"],"title":"Interpretations of microbial community studies are biased by the selected 16S rRNA gene amplicon sequencing pipeline.","type":"publication"},{"authors":["Erlangga Z","Wolff K","Poth T","Peltzer A","Nahnsen S","Spielberg S","Timrott K","Woller N","Kühnel F","Manns MP","Saborowski A","Vogel A","Saborowski M"],"categories":null,"content":"","date":1574982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574982000,"objectID":"d000062c53d14f7c9bb11e5ad6ce0292","permalink":"https://apeltzer.github.io/publication/12-antitumor/","publishdate":"2019-11-29T00:00:00+01:00","relpermalink":"/publication/12-antitumor/","section":"publication","summary":"Gallbladder cancer is associated with a dismal prognosis, and accurate in vivo models will be elemental to improve our understanding of this deadly disease and develop better treatment options. We have generated a transplantation-based murine model for gallbladder cancer that histologically mimics the human disease, including the development of distant metastasis. Murine gallbladder–derived organoids are genetically modified by either retroviral transduction or transfection with CRISPR/Cas9 encoding plasmids, thereby allowing the rapid generation of complex cancer genotypes. We characterize the model in the presence of two of the most frequent oncogenic drivers—Kras and ERBB2—and provide evidence that the tumor histology is highly dependent on the driver oncogene. Further, we demonstrate the utility of the model for the preclinical assessment of novel therapeutic approaches by showing that liposomal Irinotecan (Nal-IRI) is retained in tumor cells and significantly prolongs the survival of gallbladder cancer–bearing mice compared to conventional irinotecan.","tags":["Organoids","gallbladder","CRISPR/Cas9","Nal-IRI","mouse model"],"title":"Potent Antitumor Activity of Liposomal Irinotecan in an Organoid- and CRISPR-Cas9-Based Murine Model of Gallbladder Cancer","type":"publication"},{"authors":["Bichmann L; Nelde A; Ghosh M; Heumos L; Mohr C; Peltzer A;, Kuchenbecker L; Sachsenberg T; Walz JS; Stevanović S; Rammensee HG; Kohlbacher O"],"categories":null,"content":"","date":1570399200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570399200,"objectID":"333109c9f71d6a7adecc650b7034bbd4","permalink":"https://apeltzer.github.io/publication/11-mhcquant/","publishdate":"2019-10-07T00:00:00+02:00","relpermalink":"/publication/11-mhcquant/","section":"publication","summary":"Personalized multi-peptide vaccines are currently discussed intensively for tumor immunotherapy. In order to identify epitopes - short, immunogenic peptides - suitable for eliciting a tumor-specific immune response, human leukocyte antigen (HLA)-presented peptides are isolated by immunoaffinity purification from cancer tissue samples and analyzed by liquid chromatography-coupled tandem mass spectrometry (LC-MS/MS). Here, we present MHCquant, a fully automated, portable computational pipeline able to process LC-MS/MS data automatically and generate annotated, FDR-controlled lists of (neo-)epitopes with associated relative quantification information. We could show that MHCquant achieves higher sensitivity than established methods. While obtaining the highest number of unique peptides, the rate of predicted MHC binders remains still comparable to other tools.","tags":["mhcquant","Nextflow","FAIR","data analysis","proteomics"],"title":"MHCquant: Automated and reproducible data analysis for immunopeptidomics","type":"publication"},{"authors":null,"categories":null,"content":"","date":1568879100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568879100,"objectID":"bbbfa29d6c1dcec03401387bb58a1249","permalink":"https://apeltzer.github.io/talk/migrating-legacy-workflows-to-nextflow/","publishdate":"2019-09-19T09:45:00+02:00","relpermalink":"/talk/migrating-legacy-workflows-to-nextflow/","section":"event","summary":"Many researchers and institutions face similar issues: Having in-house legacy workflows written in bash or other formats and now facing issues with reproducibility, maintenance of these pipelines and the increasing computational demands poses severe threats to being able to address modern computational questions using such legacy workflows. In this talk, I intend to highlight the efforts we took at the QBIC to maintain compatibility between existing workflows but simultaneously porting all of our existing legacy pipelines to the Nextflow and nf-core frameworks to be able to answer these threats. With a specific application case on one of the most widely used ancient DNA pipelines, I intend to highlight the benefits of these migrated pipeline in comparison to the previously existing workflow in a 1:1 setting.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing","migrating","workflows"],"title":"Migrating legacy workflows to Nextflow","type":"event"},{"authors":null,"categories":null,"content":"","date":1567501200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567501200,"objectID":"b7f99c1fcc66a0174bd978ce7faad5ed","permalink":"https://apeltzer.github.io/talk/bigdata-analytics-with-nextflow-and-nf-core/","publishdate":"2019-09-03T11:00:00+02:00","relpermalink":"/talk/bigdata-analytics-with-nextflow-and-nf-core/","section":"event","summary":"The standardization, portability, and reproducibility of analysis pipelines is a renowned problem within the bioinformatics community. In the past, bioinformatic analysis pipelines have often been designed to work on-premise, deeply integrated into the local infrastructure and did show a customized architecture style. Because of this tight coupling of software to its surrounding environment, the resulting pipelines provided poor portability and reproducibility. Nextflow is a system that is able to provide functionality to make analysis reusability, portability and reproducibility complete, with built-in support for most computational infrastructures and container technologies such as Docker, Conda and Singularity. nf-core is a community effort to implement and collect Nextflow pipelines based on community best practices and tools. The guidelines and templates provided by the nf-core community along with a detailed documentation enable users to add new workflows and get started with Nextflow seamlessly. The outcome is a set of high-quality bioinformatics pipelines that researchers can apply broadly across various institutions and research facilities, as all workflows share common usage patterns and robust community support. Our primary goal is to provide a community-drivenplatform for a high-quality set of reproducible bioinformatics pipelines that researchers can utilize across various Institutions and research facilities.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing"],"title":"BigData analytics with Nextflow and nf-core","type":"event"},{"authors":["Fillinger S, de la Garza L, Peltzer A, Kohlbacher O, Nahnsen S."],"categories":null,"content":"","date":1566943200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566943200,"objectID":"d1d581bd5a4df50a2b1a58b6cebecf4c","permalink":"https://apeltzer.github.io/publication/10-bigdata/","publishdate":"2019-08-28T00:00:00+02:00","relpermalink":"/publication/10-bigdata/","section":"publication","summary":"Big data has been reported to be revolutionizing many areas of life, including science. It summarizes data that is unprecedentedly large, rapidly generated, heterogeneous, and hard to accurately interpret. This availability has also brought new challenges: How to properly annotate data to make it searchable? What are the legal and ethical hurdles when sharing data? How to store data securely, preventing loss and corruption? The life sciences are not the only disciplines that must align themselves with big data requirements to keep up with the latest developments. The large hadron collider, for instance, generates research data at a pace beyond any current biomedical research center. There are three recent major coinciding events that explain the emergence of big data in the context of research: the technological revolution for data generation, the development of tools for data analysis, and a conceptual change towards open science and data. The true potential of big data lies in pattern discovery in large datasets, as well as the formulation of new models and hypotheses. Confirmation of the existence of the Higgs boson, for instance, is one of the most recent triumphs of big data analysis in physics. Digital representations of biological systems have become more comprehensive. This, in combination with advances in machine learning, creates exciting new research possibilities. In this paper, we review the state of big data in bioanalytical research and provide an overview of the guidelines for its proper usage.","tags":["BigData","Nextflow","FAIR","data analysis","Review"],"title":"Challenges of big data integration in the life sciences","type":"publication"},{"authors":null,"categories":null,"content":"","date":1564124400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564124400,"objectID":"4a8fc586805e3a7803b040c45bae96a7","permalink":"https://apeltzer.github.io/talk/nf-core-community-built-bioinformatics-pipelines/","publishdate":"2019-07-26T09:00:00+02:00","relpermalink":"/talk/nf-core-community-built-bioinformatics-pipelines/","section":"event","summary":"The standardization, portability, and reproducibility of analysis pipelines is a renowned problem within the bioinformatics community. In the past, bioinformatic analysis pipelines have often been designed to work on-premise, deeply integrated into the local infrastructure and did show a customized architecture style. Because of this tight coupling of software to its surrounding environment, the resulting pipelines provided poor portability and reproducibility. Nextflow is a system that is able to provide functionality to make analysis reusability, portability and reproducibility complete, with built-in support for most computational infrastructures and container technologies such as Docker, Conda and Singularity. nf-core is a community effort to implement and collect Nextflow pipelines based on community best practices and tools. The guidelines and templates provided by the nf-core community along with a detailed documentation enable users to add new workflows and get started with Nextflow seamlessly. The outcome is a set of high-quality bioinformatics pipelines that researchers can apply broadly across various institutions and research facilities, as all workflows share common usage patterns and robust community support. Our primary goal is to provide a community-drivenplatform for a high-quality set of reproducible bioinformatics pipelines that researchers can utilize across various Institutions and research facilities.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing"],"title":"nf-core: Community built bioinformatics pipelines","type":"event"},{"authors":null,"categories":null,"content":"","date":1564057200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564057200,"objectID":"9d1f0177d39ac8f3940e10d7d8d30ac8","permalink":"https://apeltzer.github.io/talk/nf-core-community-built-bioinformatics-pipelines/","publishdate":"2019-07-25T14:20:00+02:00","relpermalink":"/talk/nf-core-community-built-bioinformatics-pipelines/","section":"event","summary":"The standardization, portability, and reproducibility of analysis pipelines is a renowned problem within the bioinformatics community. In the past, bioinformatic analysis pipelines have often been designed to work on-premise, deeply integrated into the local infrastructure and did show a customized architecture style. Because of this tight coupling of software to its surrounding environment, the resulting pipelines provided poor portability and reproducibility. Nextflow is a system that is able to provide functionality to make analysis reusability, portability and reproducibility complete, with built-in support for most computational infrastructures and container technologies such as Docker, Conda and Singularity. nf-core is a community effort to implement and collect Nextflow pipelines based on community best practices and tools. The guidelines and templates provided by the nf-core community along with a detailed documentation enable users to add new workflows and get started with Nextflow seamlessly. The outcome is a set of high-quality bioinformatics pipelines that researchers can apply broadly across various institutions and research facilities, as all workflows share common usage patterns and robust community support. Our primary goal is to provide a community-drivenplatform for a high-quality set of reproducible bioinformatics pipelines that researchers can utilize across various Institutions and research facilities.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing"],"title":"nf-core: Community built bioinformatics pipelines","type":"event"},{"authors":null,"categories":null,"content":"","date":1562403600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562403600,"objectID":"9b644b251a4df3a211f27ad8f85a513c","permalink":"https://apeltzer.github.io/talk/bigdata-analytics-with-nextflow-nf-core/","publishdate":"2019-07-06T11:00:00+02:00","relpermalink":"/talk/bigdata-analytics-with-nextflow-nf-core/","section":"event","summary":"Die Standardisierung, Portabilität und Reproduzierbarkeit von Analysemethoden ist nach wie vor ein bekanntes Problem in der angewandten Informatik und hierbei insbesondere in den Lebenswissenschaften. Analysetools sind oftmals auf spezifische Infrastruktur zugeschnitten und können nur schlecht von dieser getrennt angewandt werden. Mittels Nextflow, einer domänenspezifischen Sprache auf Groovy-Basis und nf-core, einem Framework für die Anwendung von Erfahrungen aus der Entwicklung vieler stabiler Analysepipelines, versuchen wir diese Probleme konkret zu addressieren. Hierbei werden Containertechnologien, Cloud Ressourcen und eine Community aus Entwicklern rund um den Globus eingebunden um eine möglichst flexible Entwicklung dieser Plattformen zu erreichen. Im Talk werden die einzelnen Aspekte beleuchtet und detailliert beschrieben, so dass Zuhörer auch selbst in die Entwicklung solcher Analysepipelines einsteigen können.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","Cloud","Cloud Computing"],"title":"BigData Analytics with Nextflow \u0026 nf-core","type":"event"},{"authors":["Alexander Peltzer"],"categories":null,"content":"How and why I did it I was already interested in sequencing my own genome during my time as a Ph.D. student, when the first 23andme \u0026amp; other genotyping services were available publicly. After all, back then this wasn\u0026rsquo;t as exciting as I thought due to quite restricted resolution. Having access to the full genome-wide data was something I didn\u0026rsquo;t think about for a while.\nDuring last years Black Friday event, I decided to take the opportunity to pay for my own genome to be sequenced using DanteLabs WGZ \u0026ldquo;Black Friday Offer\u0026rdquo;. After the payment, I figured out that in order to get the final BAM or FastQ file formats that you need to additionally pay US$ 50 to get an external harddisk with your \u0026ldquo;real\u0026rdquo; RAW data.\nWithout that, you will receive VCF file(s) for SNPs, CNVs, SVs, INDELs and two PDFs with two individual reports for medications and health-related insights. The Bioinformatician in me was however more interested in doing everything from scratch which I\u0026rsquo;m going to explicitly elucidate here a bit upon. The data looks very similar to standard Illumina data I already have some experience with, so standard WGS/WES pipeline analysis was the way to go for looking into it in more detail. Feel free to ask me on Twitter for more details - I might update this or do several follow ups on this posting to keep people with interest in the loop.\nDelivery Upon registering and paying upfront, I got my swab kit approximately 2 weeks after payment. Immediately after sending it in, it took another about 4 weeks to get confirmation that DNA extraction went successful and I got access to the data at the website to at least download the two pdf report and my VCF files.\nUpon receiving this, it took another 6 weeks until I got my RAW data in April 2019. The harddisk has 500GB and contained raw data with compressed FastQ (split across multiple lanes), some QC files, BAM and Index for that final BAM and the called Variants found on the website as VCF files.\nFirst Analysis Steps I decided to utilize a pipeline for modern WGS sequencing to analyze my data in a single run. My choice for this job was the Sarek pipeline which can do all of the required steps for such an analysis in an easy manner, including QC, mapping, variant calling, annotation and reporting.\nStep 1: Create a TSV file First step was to create a TSV file with my data in it. I anonymized some parts here for privacy reasons but it should be quite straightforward what the columns mean.\nAlex XY 0 Alex_Normal 15180 RAW/foo15180_L01_543_1.fastq.gz RAW/foo15180_L01_543_2.fastq.gz Alex XY 0 Alex_Normal 525 RAW/foo_L01_525_1.fastq.gz RAW/foo_L01_525_2.fastq.gz Alex XY 0 Alex_Normal 526 RAW/foo_L01_526_1.fastq.gz RAW/foo_L01_526_2.fastq.gz Alex XY 0 Alex_Normal 527 RAW/foo_L01_527_1.fastq.gz RAW/foo_L01_527_2.fastq.gz Alex XY 0 Alex_Normal 528 RAW/foo_L01_528_1.fastq.gz RAW/foo_L01_528_2.fastq.gz Alex XY 0 Alex_Normal 529 RAW/foo_L01_529_1.fastq.gz RAW/foo_L01_529_2.fastq.gz Alex XY 0 Alex_Normal 530 RAW/foo_L01_530_1.fastq.gz RAW/foo_L01_530_2.fastq.gz Alex XY 0 Alex_Normal 531 RAW/foo_L01_531_1.fastq.gz RAW/foo_L01_531_2.fastq.gz Alex XY 0 Alex_Normal 532 RAW/foo_L01_532_1.fastq.gz RAW/foo_L01_532_2.fastq.gz  Step 2: Run Mapping / QC Second, I started the mapping and initial QC for the data:\nnextflow run SciLifeLab/Sarek/main.nf -r dev -profile standard,docker --genome 'GRCh37' --sample sarek.tsv -resume  Step 3: Run VariantCalling As I was uncertain which VariantCallers would give me best results, I just selected all of the variant callers available in Sarek for germline data:\nnextflow run SciLifeLab/Sarek/germlineVC.nf -profile standard,docker -r dev --genome 'GRCh37' --sample Preprocessing/Recalibrated/recalibrated.tsv --tools 'HaplotypeCaller,strelka,manta' -resume  Step 4: Annotate the results \u0026amp; create a report Having only the raw variants isn\u0026rsquo;t really useful without annotation:\nnextflow run SciLifeLab/Sarek/annotate.nf -profile standard,docker -r dev --genome 'GRCh37' --annotateTools 'HaplotypeCaller,strelka,manta' --tools 'snpEff' nextflow run SciLifeLab/Sarek/runMultiQC.nf -r dev -profile standard,docker  Some basic statistics and information General QC The data looks quite good in terms of basic metrics on the raw data.\nAs I mapped against the GRCh37 genome reference, all these reported values are against that. In total, 936 Million reads were properly mapped, with a 41%GC content. A total coverage of 31X was achieved, with 90.6% of the genome covered at least with 10X, and 56.7% of the genome covered with at least 30X as reported by the Sarek reports.\nHLATyping I also extracted all of the reads on Chromosome 6:\nsamtools view -bh -o chr6.alex.bam input.bam \u0026quot;6\u0026quot; nextflow run nf-core/hlatyping --bam -profile docker --reads 'chr6.alex.bam'  and HLA-typed myself using the nf-core/hlatyping pipeline. Might be just the same curiousity but this worked as well using the data provided and provided me with my HLA-A, HLA-B and HLA-C types. Doing this wasn\u0026rsquo;t possible with the 23andme test back then for example.\nmtDNA / Y Typing Using a simple command, I was able to extract and call the consensus sequence of my mtDNA and type myself to confirm my previous 23and me genotyping results. Fortunately, the Haplogrep 2 service and the Y-Haplo teams provide their services for everyone and uploading my data there to determine my mtDNA haplogroup and my Y-haplogroup was quit easy, too.\nFuture updates There will be follow up posts once I got more time to look at my data. I will probably integrate my data with population genetics datasets such as Human Origins and also have a more detailed look at some variant analysis methods.\n","date":1556402400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556474400,"objectID":"af319ce9eb44e415b8fda9396025eef6","permalink":"https://apeltzer.github.io/post/02-dante-wgs/","publishdate":"2019-04-28T00:00:00+02:00","relpermalink":"/post/02-dante-wgs/","section":"post","summary":"Analyzing my own genome data from DanteLabs WGZ service.","tags":["Bioinformatics","NF-Core","Nextflow","WGS","Genetics"],"title":"Affordable WGS for personal usage (part 1)","type":"post"},{"authors":["Ewels AP, Peltzer A, Fillinger S, Alneberg J, Patel H, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S"],"categories":null,"content":"","date":1555365600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555365600,"objectID":"d99a660f564139e3730341aac352670f","permalink":"https://apeltzer.github.io/publication/09-nfcore/","publishdate":"2019-04-16T00:00:00+02:00","relpermalink":"/publication/09-nfcore/","section":"publication","summary":"The standardization, portability, and reproducibility of analysis pipelines is a renowned problem within the bioinformatics community. Bioinformatic analysis pipelines are often designed for execution on-premise, and this inevitably leads to a level of customisation and integration that is only applicable to the local infrastructure. More notably, the software required to run these pipelines is also tightly coupled with the local compute environment, and this leads to poor pipeline portability, and reproducibility of the ensuing results - both of which are fundamental requirements for the validation of scientific findings. Here we introduce nf-core, a framework that provides a community-driven platform for the creation and development of best practice analysis pipelines written in the Nextflow language. Nextflow has built-in support for pipeline execution on most computational infrastructures, as well as automated deployment using container technologies such as Conda, Docker, and Singularity. Therefore, key obstacles in pipeline development such as portability, reproducibility, scalability and unified parallelism are inherently addressed by all nf-core pipelines. Furthermore, to ensure that new pipelines can be added seamlessly, and existing pipelines are able to inherit up-to-date functionality the nf-core community is actively developing a suite of tools that automate pipeline creation, testing, deployment and synchronization. The peer-review process during pipeline development ensures that best practices and common usage patterns are imposed and therefore, adhere to community guidelines. Our primary goal is to provide a community-driven platform for high-quality, excellent documented and reproducible bioinformatics pipelines that can be utilized across various institutions and research facilities.","tags":["nf-core","Nextflow","FAIR","data analysis"],"title":"nf-core: Community curated bioinformatics pipelines","type":"publication"},{"authors":null,"categories":null,"content":"","date":1551276000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551276000,"objectID":"1aa9d553edc8f559b1fc7e08dd57915d","permalink":"https://apeltzer.github.io/talk/complex-genomics-analysis-pipelines-made-simple-with-nextflow-aws-batch/","publishdate":"2019-02-27T15:00:00+01:00","relpermalink":"/talk/complex-genomics-analysis-pipelines-made-simple-with-nextflow-aws-batch/","section":"event","summary":"The science of genomics is increasingly contributing to the sort of insights which lead to longer and healthier lives for us all. But the computational pipelines needed to support accurate and timely analysis of huge quantities of genomic data are truly extreme - more than enough to break the most well configured on-premise HPC facility. The key factor? A lack of elasticity: no ability to stretch the infrastructure a million different ways to accommodate needs that change from minute to minute and just aren't predictable. When this happens, whole collections of analysis jobs get piled up behind each other, similar to a traffic jam on a complex road network. We can't fix the traffic on the roads, but for genomics, we have tools like NextFlow.io (a popular open source data-driven pipeline tool for orchestrating scientific pipelines) and AWS Batch, which have access to Amazon EC2 and all the elasticity a scientist could want. Thanks to all these technologies, our customer QBiC Tübingen is able to put all their efforts into their number one mission: to improve patient health by translating our understanding of the Human Gut Microbiome into knowledge and techniques for creating innovative new medicines that will help us all. You'll learn about: • QBiC's use of NextFlow's to manage complex pipelines and orchestrate interdependent jobs; • Their reasons for choosing AWS and the steps they took to build trust in us; • NextFlow's integration with AWS Batch which dynamically expands and contracts cloud resources to meet the needs of the pipeline as it works its way through the data. Suited for: HPC Users, Medical IT and the engineers who support them.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","AWS","Cloud","Cloud Computing"],"title":"Complex Genomics Analysis Pipelines made Simple with NextFlow \u0026 AWS Batch","type":"event"},{"authors":null,"categories":null,"content":"","date":1551088800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551088800,"objectID":"58b4deaf61c730e682fd8a385c512ad7","permalink":"https://apeltzer.github.io/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","publishdate":"2019-02-25T11:00:00+01:00","relpermalink":"/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","section":"event","summary":"A basic introduction into nf-core and the aims and goals behind the project to make pipeline development a community mission across multiple bionformatics core units across the globe using Nextflow.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"nf-core: Community-based best practice pipeline development in Nextflow","type":"event"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://apeltzer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1541512800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541512800,"objectID":"7a8aa0d34b48c4e5f7ad49053a0f485d","permalink":"https://apeltzer.github.io/talk/bioinfo-core-teleconference-on-nextflow-and-nf-core/","publishdate":"2018-11-06T15:00:00+01:00","relpermalink":"/talk/bioinfo-core-teleconference-on-nextflow-and-nf-core/","section":"event","summary":"After the ISMB workshop in Chicago, I gave a teleconference talk running a demo on Nextflow and nf-core. Video, Slides and Webpage available here.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"bioinfo-core teleconference on Nextflow and nf-core","type":"event"},{"authors":null,"categories":null,"content":"","date":1540807200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540807200,"objectID":"d2e0311ad6093924ec5f54dca6ce1750","permalink":"https://apeltzer.github.io/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","publishdate":"2018-10-29T11:00:00+01:00","relpermalink":"/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","section":"event","summary":"I gave a talk about our efforts in the nf-core project on making pipeline development a community mission across multiple bionformatics core units across the globe using Nextflow.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"nf-core: Community-based best practice pipeline development in Nextflow","type":"event"},{"authors":null,"categories":null,"content":"","date":1539590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539590400,"objectID":"15c5935ca9c17170b6f2d246caca1a2b","permalink":"https://apeltzer.github.io/talk/eager2-a-modern-framework-for-adna-analysis/","publishdate":"2018-10-15T10:00:00+02:00","relpermalink":"/talk/eager2-a-modern-framework-for-adna-analysis/","section":"event","summary":"Ancient DNA datasets generated today are getting bigger and bigger. I introduced the start of a community pipeline called EAGER2 to accommodate life-science researchers with an interest in aDNA to collaborate on developing a reproducible and cloud-compatible pipeline for a successfull analysis on aDNA.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core","aDNA","ancient DNA","EAGER2"],"title":"EAGER2: A modern framework for aDNA analysis","type":"event"},{"authors":null,"categories":null,"content":"Slides can be found here.\n","date":1537790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537790400,"objectID":"285d127479e6573fe359ada3bc2c2b27","permalink":"https://apeltzer.github.io/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","publishdate":"2018-09-24T14:00:00+02:00","relpermalink":"/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","section":"event","summary":"Introducing the efforts in the nf-core project and in containerizing applications for genomics workflows across multiple bioinformatics core units.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"nf-core: Community-based best practice pipeline development in Nextflow","type":"event"},{"authors":null,"categories":null,"content":"Over at nf-co.re we are working on solving one of the biggest issues in modern computational biology: How do we make the efforts to analyze various kinds of OMICs data reproducible and thus also accessible to everyone? In the age of \u0026ldquo;fake news\u0026rdquo;, science has to not only answer questions, but also provide means that their findings can be reproduced and not mistakenly used for other malicious or harmful activities. While doing malicious things with OMICs data is certainly hard to pursue, the good practice of making entire analysis procedures reproducible - even years later - remains the biggest motivators for the nf-core project. Our intention is to form a collaborative research platform to enable various bioinformatics researchers to benefit from know-how as generated in this project.\n","date":1535025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535025600,"objectID":"fed64c0c19329b7310402fe06bbe198a","permalink":"https://apeltzer.github.io/project/nfcore/","publishdate":"2018-08-23T14:00:00+02:00","relpermalink":"/project/nfcore/","section":"project","summary":"Community-based best-practice workflows in Nextflow","tags":["Bioinformatics","Nextflow","nf-core"],"title":"nf-core","type":"project"},{"authors":["Alexander Peltzer","Tobias Koch"],"categories":null,"content":"This document is based on experiences provided by Maxime Garcia at his blogpost on running CAW with AWSBatch and our own experiences. We just wanted to acknowledge the effort that Maxime also put into making this work!\nBasic Requirements In order to run this, you need to have a AWS account set up and create an IAM user for it. More detailed information on how to do this can be found under the above link. The IAM set up is described in the next section.\nIAM User setup Please follow the instructions on AWS to set up an IAM user for running Batch jobs on AWSBatch. This user has to be provided with the required permissions to run Batch jobs. Permissions on AWS are set up using the IAM service and you have to start by creating a new user using the \u0026ldquo;Add user\u0026rdquo; interface. Please attach the following permissions to your newly created user afterwards:\n AmazonEC2FullAccess AmazonS3FullAccess AWSBatchFullAccess  This should already suffice in terms of user permissions to run jobs on AWS Batch.\nService role setup Next, we need to set up permissions for AWS Batch to e.g. run/stop EC2 instances for us by generating the required roles on IAM. These are managed as separate roles in IAM under \u0026ldquo;Roles\u0026rdquo;. Some of these are created automatically for you when you set up a Batch job at the first time, but we can make sure that these are already present and configured properly beforehand.\nYou need the service roles:\n AWSBatchServiceRole ecsInstanceRole AWSServiceRoleForEC2SpotFleet AmazonEC2SpotFleetRole  The latter two are not required if you are not intending to use spot instances (you should use them to save money!).\nAWSBatchServiceRole This role should have the policy AWSBatchServiceRole attached. ecsInstanceRole This role should have the policies AmazonS3FullAccess and AmazonEC2ContainerServiceforEC2Role set.\nAWSServiceRoleForEC2SpotFleet This role should have the policy AWSEC2SpotFleetServiceRolePolicy set.\nAmazonEC2SpotFleetRole This role should have the policies AmazonEC2SpotFleetRole and AmazonEC2SpotFleetTaggingRole set accordingly.\nOnce you have set these roles, the permissions for running a job with the selected IAM user can be used to configure a compute environment (CE) and a job queue to run your jobs.\nYou need to get the accesskey/token combination for using AWS and need to have these present on the machine starting the jobs in a file ~/.aws/credentials such as:\n[default] aws_access_key_id = KEY aws_secret_access_key = secretKEY  AMI Now that you have all the permissions set up you need to create a custom amazon machine image (AMI). This will be used later by the EC2 instances started by AWS batch.\nAMI preparation First you have to set up an EC2 instance which will later be converted into an AMI. Please use the ECS-Optimized Amazon Linux AMI since it provides docker installation and configuration.\nChoose a t2.micro instance for the instance type since the instance type does not impact the AMI.\nConfiguration of the Instance Details is not needed as the default configuration should suffice.\nDepending on the docker image sizes you expect your Batch instances to handle, choose a larger EBS storage in the storage configuration.\nOptional Tags can be added in Step 5.\nIn the last step configure your Security Group. Make sure you can connect to the EC2 instance when doing so. You can also let AWS create a security group for you.Now launch the instance.\nConnect to your t2.micro instance and perform the following steps:\n Update the yum repository sudo yum update Check the docker configuration and make sure the docker storage size matches your configured one. docker info Check whether awscli is installed aws --version Install awscli using miniconda  wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86\\_64.sh bash Miniconda3-latest-Linux-x86\\_64.sh -p /home/ec2-user/miniconda /home/ec2-user/miniconda/bin/conda install -c conda-forge awscli /home/ec2-user/miniconda/bin/aws --version  AMI creation Now that you have configured your EC2 instance you can logout and stop the instance.\nIn the EC2 Management Console select the stopped instance and choose the Action Image-\u0026gt;Create Image\nChoose a name and a description and create the AMI.\nSet up QUEUE / Compute Environment In order to submit jobs to AWS Batch, you need to have a working compute environment (CE) and a JobQueue set up. Start with the CE and then create a compute environment.\nCompute Environment  Log in to AWS Batch Navigation menu: \u0026ldquo;Compute environments\u0026rdquo; \u0026ldquo;Create a new compute environment\u0026rdquo; Select \u0026ldquo;Managed\u0026rdquo; Provide a name for the Compute Environment Service role AWSBatchServiceRole Instance role ecsInstanceRole EC2 key pair: The key pair of the IAM user you intend to use for running AWS Batch jobs Select \u0026ldquo;on-demand\u0026rdquo; or \u0026ldquo;spot\u0026rdquo; (spot is cheaper) Select Maximum Price you\u0026rsquo;re willing to pay for spot instances Spot fleet role AmazonEC2SpotFleetRole Enable user-specified AMI id and provide the AMI id you created If desired, attach a Key/Value pair to make it possible to later generate e.g. billing information on a per compute environment basis  Select create and you\u0026rsquo;re all set!\nJobQueue set up  Log in to AWS Batch Select \u0026ldquo;Job queues\u0026rdquo; in Navigation menu Provide a name you remember (!) for your job queue Set a priority Select the previously created compute environment for running jobs  Select create and you\u0026rsquo;re all set!\nAWS Configuration Nextflow AWS Batch configuration in Nextflow requires a couple of things:\n Publicly accessible Docker containers per process or per pipeline Executor set to awsbatch JobQueue set to an enabled AWS Batch JobQueue An AWS region, e.g. 'us-east-1' AWSCli present in the AMI which runs the jobs S3 Buckets for temporary/work files and results  An example on how to deal with things is to ask your users to specify the missing parts on pipeline execution. A working example is set in ICGC-featureCounts where the awsbatch.config specifies the required parameters for execution on AWSBatch.\n workDir Executor Queue Region And a custom path to AWScli inside the customized AMI to schedule jobs  This is then set to default values in the nextflow.config of the pipeline and set according to user specified parameters when executing the pipeline. This way, users don\u0026rsquo;t need to change the awsbatch.config file but can instead rely on using a set of parameters --workDir, --awsqueue, --awsregion and should be fine.\nRunning a basic job with AWS Batch You can execute an AWSBatch job on your local workstation or on a running EC2 instance. For longer running jobs, it might make sense to use a small EC2 instance (t2.micro) to run your job, as you\u0026rsquo;re not relying on network connections between your local workstation and the AWS Batch CE/JobQueue then.\nYou can use nextflow cloud create \u0026lt;your-cloud-name\u0026gt; to launch a simple instance that has nextflow installed to create a simple instance for running AWS Batch jobs.\nFor this instance additional configuration can be provided in your nextflow.configor ~/.nextflow/configfile.\ncloud { imageId = 'ami-xxxx' instanceType = 't2.micro' keyName = \u0026lt;AWS keyname\u0026gt; userName = 'ec2-user' }  Troubleshooting I don\u0026rsquo;t want to risk requesting instances when nothing runs anymore - what should I do? Simply disable the JobQueue and the Compute Environment and nothing will be requested when there are no jobs running. In our experience, failing jobs will also result in instance termination.\nI have configured nextflow according to your suggestion but every job fails with Essential container in task exited - how can I fix that? Open CloudWatch and look at the log for the failed task. If the error is aws command not found or bash: /home/ec2-user/miniconda/bin/aws: No such file or directory' then make sure aws is installed and there are no typos in your nextflow.config.\nI found an AMI option for the nextflow.config file but it doesn\u0026rsquo;t work - is it broken? No, the cloud.ami configuration is only used for cluster creation. AWS Batch is a different service and the AMI needs to be specified inside the compute environment.\nThere is no --workDir option. Where can I find it? The --workDir, --awsqueue ,--awsregion options are provided by the pipeline. If your pipeline is no nf-core pipeline those options won\u0026rsquo;t be present. You would have to specify queue and region in your nextflow.config file. You can also provide the work directory with the nextflow parameter -work-dir / -w.\n","date":1534802400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534968900,"objectID":"82b44e37b1fb23d5cf121c1e9b82fc20","permalink":"https://apeltzer.github.io/post/01-aws-nfcore/","publishdate":"2018-08-21T00:00:00+02:00","relpermalink":"/post/01-aws-nfcore/","section":"post","summary":"How to run AWSBatch with NF-Core pipelines using Nextflow.","tags":["Bioinformatics","NF-Core","Nextflow","AWS","Cloud Computing"],"title":"Running nf-co.re pipelines with AWSBatch","type":"post"},{"authors":null,"categories":null,"content":"Slides can be found here.\n","date":1531778400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531778400,"objectID":"69d5c93b73bf58454cbabd8ca65fb9d9","permalink":"https://apeltzer.github.io/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","publishdate":"2018-07-17T00:00:00+02:00","relpermalink":"/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","section":"event","summary":"Introducing the efforts in the nf-core project and in containerizing applications for genomics workflows across multiple bioinformatics core units.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"nf-core: Community-based best practice pipeline development in Nextflow","type":"event"},{"authors":null,"categories":null,"content":"Slides can be found here.\n","date":1530828000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530828000,"objectID":"46ea35dfbb196d85a0ab7d5c1ae8d830","permalink":"https://apeltzer.github.io/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","publishdate":"2018-07-06T00:00:00+02:00","relpermalink":"/talk/nf-core-community-based-best-practice-pipeline-development-in-nextflow/","section":"event","summary":"I gave a talk about our efforts in the nf-core project on making pipeline development a community mission across multiple bionformatics core units across the globe using Nextflow.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"nf-core: Community-based best practice pipeline development in Nextflow","type":"event"},{"authors":null,"categories":null,"content":"Slides can be found here.\n","date":1530136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530136800,"objectID":"19d3fceec080ce7a13f0a4641a9403a4","permalink":"https://apeltzer.github.io/talk/reproducible-data-analysis-with-nextflow-nf-core/","publishdate":"2018-06-28T00:00:00+02:00","relpermalink":"/talk/reproducible-data-analysis-with-nextflow-nf-core/","section":"event","summary":"I gave a talk about our efforts in the nf-core project on making pipeline development a community mission across multiple bionformatics core units across the globe using Nextflow.","tags":["reproducible research","Bioinformatics","Nextflow","Nf-core"],"title":"Reproducible data analysis with Nextflow \u0026 nf-core","type":"event"},{"authors":["Peltzer et al"],"categories":null,"content":"","date":1517439600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517439600,"objectID":"e53d3eadef87df8723f5a069fda17e84","permalink":"https://apeltzer.github.io/publication/08-baehr/","publishdate":"2018-02-01T00:00:00+01:00","relpermalink":"/publication/08-baehr/","section":"publication","summary":"For historic individuals, the outward appearance and other phenotypic characteristics remain often non-resolved. Unfortunately, images or detailed written sources are only scarcely available in many cases. Attempts to study historic individuals with genetic data so far focused on hypervariable regions of mitochondrial DNA and to some extent on complete mitochondrial genomes. To elucidate the potential of in-solution based genome-wide SNP capture methods - as now widely applied in population genetics - we extracted DNA from the 17th century remains of George Bähr, the architect of the Dresdner Frauenkirche. We were able to identify the remains to be of male origin, showing sufficient DNA damage, deriving from a single person and being thus likely authentic. Furthermore, we were able to show that George Bähr had light skin pigmentation and most likely brown eyes. His genomic DNA furthermore points to a Central European origin. We see this analysis as an example to demonstrate the prospects that new in-solution SNP capture methods can provide for historic cases of forensic interest, using methods well established in ancient DNA (aDNA) research and population genetics.","tags":["historic individuals","aDNA","ancient DNA"],"title":"Inferring genetic origins and phenotypic traits of George Bähr, the architect of the Dresden Frauenkirche","type":"publication"},{"authors":["Skoglund et al"],"categories":null,"content":"","date":1505944800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505944800,"objectID":"50b48d6d9728933eb4d3af36848b6c05","permalink":"https://apeltzer.github.io/publication/07-african/","publishdate":"2017-09-21T00:00:00+02:00","relpermalink":"/publication/07-african/","section":"publication","summary":"We assembled genome-wide data from 16 prehistoric Africans. We show that the anciently divergent lineage that comprises the primary ancestry of the southern African San had a wider distribution in the past, contributing ~2/3 of the ancestry of Malawi hunter-gatherers ~8100-2500 years ago, and ~1/3 of Tanzanian hunter-gatherers ~1400 years ago. We document how the spread of farmers from western Africa involved complete replacement of local hunter-gatherers in some regions, and we track the spread of herders by showing that the population of a ~3100 year-old pastoralist from Tanzania contributed ancestry to people from northeast to southern Africa, including a ~1200-year-old southern African pastoralist. The deepest diversifications of African lineages were complex, involving long-distance gene flow, or a lineage more deeply diverging than that of the San contributing more to some western Africans than others. We finally leverage ancient genomes to document episodes of natural selection in southern African populations.","tags":["African","aDNA","ancient DNA"],"title":"Reconstructing Prehistoric African Population Structure","type":"publication"},{"authors":null,"categories":null,"content":"","date":1505685600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505685600,"objectID":"777b9fc39b1d63c73963a863b6e12ae3","permalink":"https://apeltzer.github.io/talk/modern-computational-methods-in-ancient-dna-analysis/","publishdate":"2017-09-18T00:00:00+02:00","relpermalink":"/talk/modern-computational-methods-in-ancient-dna-analysis/","section":"event","summary":"I jointly organized a workshop on modern computational methods in ancient DNA analysis with Judith Neukamm and Gabriel Renaud. See [here](http://gcb2017.de/?page_id=328) for more details.","tags":["ancient DNA","Bioinformatics"],"title":"Modern computational methods in ancient DNA analysis","type":"event"},{"authors":null,"categories":null,"content":"","date":1504389600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504389600,"objectID":"de1f4ce23044cb587dbb86feeee4a9d0","permalink":"https://apeltzer.github.io/talk/mitobench-and-mitodb-novel-interactive-methods-for-population-genetics-on-mitochondrial-dna/","publishdate":"2017-09-03T00:00:00+02:00","relpermalink":"/talk/mitobench-and-mitodb-novel-interactive-methods-for-population-genetics-on-mitochondrial-dna/","section":"event","summary":"I introduced our novel tools mitoBench and the mitoDB application, aiming at making population genetics analysis easier for mitochondrial DNA.","tags":["ancient DNA","mitoBench","mitoDB"],"title":"MitoBench and MitoDB: Novel interactive methods for population genetics on mitochondrial DNA","type":"event"},{"authors":["Lazaridis et al"],"categories":null,"content":"","date":1501624800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501624800,"objectID":"3f34e96bdaddaad3ab1814f305ed9335","permalink":"https://apeltzer.github.io/publication/06-greeks/","publishdate":"2017-08-02T00:00:00+02:00","relpermalink":"/publication/06-greeks/","section":"publication","summary":"The origins of the Bronze Age Minoan and Mycenaean cultures have puzzled archaeologists for more than a century. We have assembled genome-wide data from 19 ancient individuals, including Minoans from Crete, Mycenaeans from mainland Greece, and their eastern neighbours from southwestern Anatolia. Here we show that Minoans and Mycenaeans were genetically similar, having at least three-quarters of their ancestry from the first Neolithic farmers of western Anatolia and the Aegean, and most of the remainder from ancient populations related to those of the Caucasus and Iran. However, the Mycenaeans differed from Minoans in deriving additional ancestry from an ultimate source related to the hunter-gatherers of eastern Europe and Siberia, introduced via a proximal source related to the inhabitants of either the Eurasian steppe or Armenia. Modern Greeks resemble the Mycenaeans, but with some additional dilution of the Early Neolithic ancestry. Our results support the idea of continuity but not isolation in the history of populations of the Aegean, before and after the time of its earliest civilizations.","tags":["Minoan","Greek","aDNA","ancient DNA"],"title":"Genetic origins of the Minoans and Mycenaeans","type":"publication"},{"authors":["Schuenemann \u0026 Peltzer et al"],"categories":null,"content":"","date":1496095200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496095200,"objectID":"e362359d65d6d84f8225324b5100a146","permalink":"https://apeltzer.github.io/publication/05-mummies/","publishdate":"2017-05-30T00:00:00+02:00","relpermalink":"/publication/05-mummies/","section":"publication","summary":"Egypt, located on the isthmus of Africa, is an ideal region to study historical population dynamics due to its geographic location and documented interactions with ancient civilizations in Africa, Asia, and Europe. Particularly, in the first millennium BCE Egypt endured foreign domination leading to growing numbers of foreigners living within its borders possibly contributing genetically to the local population. Here we mtDNA and nuclear DNA from mummified humans recovered from Middle Egypt that span around 1,300 years of ancient Egyptian history from the Third Intermediate to the Roman Period. Our analyses reveal that ancient Egyptians shared more Near Eastern ancestry than present-day Egyptians, who received additional Sub-Saharan admixture in more recent times. This analysis establishes ancient Egyptian mummies as a genetic source to study ancient human history and offers the perspective of deciphering Egypt’s past at a genome-wide level.","tags":["Egypt","evolution","aDNA","ancient DNA","Mummy"],"title":"Ancient Egyptian mummy genomes suggest an increase of Sub-Saharan African ancestry in post-Roman periods","type":"publication"},{"authors":null,"categories":null,"content":"","date":1491343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491343200,"objectID":"54f9c9f81196933ee117b30e6a7d3dae","permalink":"https://apeltzer.github.io/talk/ancient-egyptian-mummy-genomes-suggest-an-increase-of-sub-saharan-african-ancestry-in-post-roman-periods./","publishdate":"2017-04-05T00:00:00+02:00","relpermalink":"/talk/ancient-egyptian-mummy-genomes-suggest-an-increase-of-sub-saharan-african-ancestry-in-post-roman-periods./","section":"event","summary":"Conference poster on our work on ancient Egyptian mummy genomes at the UKAS 2017 conference","tags":["ancient DNA","Egypt","Mummy"],"title":"Ancient Egyptian mummy genomes suggest an increase of Sub-Saharan African ancestry in post-Roman periods.","type":"event"},{"authors":null,"categories":null,"content":"","date":1491343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491343200,"objectID":"94ad22d5f4711fb90e38689ae30719bc","permalink":"https://apeltzer.github.io/talk/mitobench-modern-methods-for-visually-enhanced-handling-of-human-mitochondrial-data-and-reference-retrieval./","publishdate":"2017-04-05T00:00:00+02:00","relpermalink":"/talk/mitobench-modern-methods-for-visually-enhanced-handling-of-human-mitochondrial-data-and-reference-retrieval./","section":"event","summary":"I gave a talk about our novel application mitoBench, describing its use for mitochondrial DNA analysis work at the UKAS conference.","tags":["ancient DNA","mitoBench","mitoDB"],"title":"mitoBench: Modern methods for visually enhanced handling of human mitochondrial data and reference retrieval.","type":"event"},{"authors":null,"categories":null,"content":"","date":1481324400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481324400,"objectID":"203845d869bb09854505633cd290ebfe","permalink":"https://apeltzer.github.io/talk/lightweight-virtualization-using-docker/","publishdate":"2016-12-10T00:00:00+01:00","relpermalink":"/talk/lightweight-virtualization-using-docker/","section":"event","summary":"","tags":["Docker","Bioinformatics","pipeline"],"title":"Lightweight virtualization using Docker","type":"event"},{"authors":["Arora et al"],"categories":null,"content":"","date":1480892400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480892400,"objectID":"f5df6189ae03282c7a45f69680c29117","permalink":"https://apeltzer.github.io/publication/04-syphilis/","publishdate":"2016-12-05T00:00:00+01:00","relpermalink":"/publication/04-syphilis/","section":"publication","summary":"The abrupt onslaught of the syphilis pandemic that started in the late fifteenth century established this devastating infectious disease as one of the most feared in human history1. Surprisingly, despite the availability of effective antibiotic treatment since the mid-twentieth century, this bacterial infection, which is caused by Treponema pallidum subsp. pallidum (TPA), has been re-emerging globally in the last few decades with an estimated 10.6 million cases in 2008 (ref. 2).","tags":["pathogens","evolution","antibiotics"],"title":"Origin of modern syphilis and emergence of a pandemic Treponema pallidum cluster","type":"publication"},{"authors":null,"categories":null,"content":"","date":1468792800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468792800,"objectID":"ee0222e332be89885983ca13a90300a8","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2016-07-18T00:00:00+02:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":["Fu et al"],"categories":null,"content":"","date":1462140000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462140000,"objectID":"7358c6c7ade2572a0f0a6c5ccf6bf58f","permalink":"https://apeltzer.github.io/publication/03-iceage/","publishdate":"2016-05-02T00:00:00+02:00","relpermalink":"/publication/03-iceage/","section":"publication","summary":"Modern humans arrived in Europe ~45,000 years ago, but little is known about their genetic composition before the start of farming ~8,500 years ago. Here we analyse genome-wide data from 51 Eurasians from ~45,000–7,000 years ago. Over this time, the proportion of Neanderthal DNA decreased from 3–6% to around 2%, consistent with natural selection against Neanderthal variants in modern humans. Whereas there is no evidence of the earliest modern humans in Europe contributing to the genetic composition of present-day Europeans, all individuals between ~37,000 and ~14,000 years ago descended from a single founder population which forms part of the ancestry of present-day Europeans.","tags":["ancient DNA","aDNA"],"title":"The genetic history of Ice Age Europe","type":"publication"},{"authors":["Peltzer et al"],"categories":null,"content":"","date":1459375200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459375200,"objectID":"d1ba2a7ea28961989a470288d6e26924","permalink":"https://apeltzer.github.io/publication/02-eager/","publishdate":"2016-03-31T00:00:00+02:00","relpermalink":"/publication/02-eager/","section":"publication","summary":"The automated reconstruction of genome sequences in ancient genome analysis is a multifaceted process. Here we introduce EAGER, a time-efficient pipeline, which greatly simplifies the analysis of large-scale genomic data sets. EAGER provides features to preprocess, map, authenticate, and assess the quality of ancient DNA samples. Additionally, EAGER comprises tools to genotype samples to discover, filter, and analyze variants. EAGER encompasses both state-of-the-art tools for each step as well as new complementary tools tailored for ancient DNA data within a single integrated solution in an easily accessible format.","tags":["ancient DNA","aDNA"],"title":"EAGER: efficient ancient genome reconstruction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1443391200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443391200,"objectID":"cc56f7a6fa722b3a333fc7231374f074","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2015-09-28T00:00:00+02:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline","reproducible research"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":null,"categories":null,"content":"","date":1436738400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436738400,"objectID":"8e91da1cd8dbcc80bebff9a24a43b1e9","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2015-07-13T00:00:00+02:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":null,"categories":null,"content":"","date":1436565600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436565600,"objectID":"b54cf4bc26430c51219a6edfeb4ee026","permalink":"https://apeltzer.github.io/talk/skin-and-hair-colour-prediction-based-on-genetic-data./","publishdate":"2015-07-11T00:00:00+02:00","relpermalink":"/talk/skin-and-hair-colour-prediction-based-on-genetic-data./","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","Evolutionary Biology"],"title":"Skin and hair colour prediction based on genetic data.","type":"event"},{"authors":null,"categories":null,"content":"","date":1414969200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414969200,"objectID":"dde9515ebc3d9e72ceaf30220bc707ca","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2014-11-03T00:00:00+01:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":null,"categories":null,"content":"","date":1409176800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409176800,"objectID":"60b1cf615eff107abb74fcf407a8d409","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2014-08-28T00:00:00+02:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":null,"categories":null,"content":"","date":1409176800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409176800,"objectID":"35cdf7cb4a426b19602c6b97ee5eaac1","permalink":"https://apeltzer.github.io/talk/eager-efficient-ancient-genome-reconstruction/","publishdate":"2014-08-28T00:00:00+02:00","relpermalink":"/talk/eager-efficient-ancient-genome-reconstruction/","section":"event","summary":"","tags":["ancient DNA","Bioinformatics","pipeline"],"title":"EAGER: Efficient ancient genome Reconstruction","type":"event"},{"authors":["Jager et al"],"categories":null,"content":"","date":1404684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404684000,"objectID":"0dc6492a3ad15025483b18b5a077eb47","permalink":"https://apeltzer.github.io/publication/01-inphap/","publishdate":"2014-07-07T00:00:00+02:00","relpermalink":"/publication/01-inphap/","section":"publication","summary":"To understand individual genomes it is necessary to look at the variations that lead to changes in phenotype and possibly to disease. However, genotype information alone is often not sufficient and additional knowledge regarding the phase of the variation is needed to make correct interpretations. Interactive visualizations, that allow the user to explore the data in various ways, can be of great assistance in the process of making well informed decisions. But, currently there is a lack for visualizations that are able to deal with phased haplotype data.","tags":[],"title":"inPHAP: Interactive visualization of genotype and phased haplotype data","type":"publication"}]